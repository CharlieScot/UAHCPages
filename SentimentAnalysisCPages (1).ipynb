{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHR6xXvUIFwp"
      },
      "source": [
        "SENTIMENT ANALYSIS OF IMDB USING CNN AND RNN\n",
        "\n",
        "Here's why using a hybrid of CNN and RNN for sentiment analysis on a dataset like IMDB is a good idea:\n",
        "\n",
        "Strengths of Individual Models:\n",
        "\n",
        "CNNs (Convolutional Neural Networks):\n",
        "\n",
        ".-Pattern Recognition: They excel at identifying sentiment-laden phrases and keywords like \"terrible acting\" or \"laugh-out-loud funny.\" These are common in movie reviews and can be strong indicators of sentiment.\n",
        "\n",
        ".-Focus on Key Words: CNNs don't necessarily need to understand the entire sentence structure, just the presence of these key sentiment indicators. This is beneficial for casual language and slang in the IMDB dataset.\n",
        "\n",
        "\n",
        "RNNs (Recurrent Neural Networks):\n",
        "\n",
        "1.-Context Awareness: Unlike CNNs, RNNs can understand the sequence and order of words in a sentence. This is crucial because sentiment can depend on word order. For instance, \"not bad\" has a different meaning than \"bad not.\"\n",
        "\n",
        "2.-Long-range Dependencies: RNNs, especially LSTMs (Long Short-Term Memory networks), can handle long-distance dependencies. Sentiment can sometimes be influenced by words far apart in a review. RNNs can learn these longer-range relationships.\n",
        "\n",
        "Benefits of the Hybrid Approach:\n",
        "\n",
        "Combined Strengths: By combining CNNs and RNNs, you leverage the advantages of both:\n",
        "CNNs capture sentiment indicators.\n",
        "RNNs provide context and understand word order's impact.\n",
        "\n",
        "Improved Accuracy: This combined approach can potentially lead to a more accurate sentiment analysis model. The model can capture both the presence of sentiment indicators and the overall flow of the review."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "malJq48xsZWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.- IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "lTLZpsQBsbOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KQI8f5ui5jDy"
      },
      "outputs": [],
      "source": [
        "# tensorflow as tf: Imports the TensorFlow library as tf for easy access.\n",
        "\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences: Imports the\n",
        "# pad_sequences function from the tensorflow.keras.preprocessing.sequence module.\n",
        "# This function is used to pad sequences of different lengths to a uniform length\n",
        "# for processing by the model.\n",
        "\n",
        "# from tensorflow.keras.models import Model: Imports the Model class from the\n",
        "# tensorflow.keras.models module. This class is used to define and build the neural\n",
        "# network architecture.\n",
        "\n",
        "# from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, LSTM,\n",
        "# Dense, Dropout: Imports various layers used for building the neural network:\n",
        "# Input: Represents the input layer of the model.\n",
        "# Embedding: Converts words into numerical vectors.\n",
        "# Conv1D: Applies one-dimensional convolutional filters to the sequence.\n",
        "# MaxPooling1D: Performs downsampling on the output of the convolutional layers.\n",
        "# LSTM: Applies Long Short-Term Memory layers to capture long-range dependencies\n",
        "# in the sequence.\n",
        "# Dense: Represents fully-connected layers for classification.\n",
        "# Dropout: Introduces dropout regularization to prevent overfitting.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "STl4-YP-srEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.- LOADING DATASET AND DIVIDING TRAIN AND TEST"
      ],
      "metadata": {
        "id": "mIW1N9_-ssYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ISbb8SzsMMwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97f5b55-5e66-4f23-b8e4-2c9f9cc8a6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Loading the Dataset:\n",
        "\n",
        "# tf.keras.datasets.imdb.load_data(): This part is responsible for loading the\n",
        "# IMDB dataset from TensorFlow's built-in datasets.\n",
        "\n",
        "# Splitting into Training and Validation Sets:\n",
        "\n",
        "# (X_train, y_train), (X_val, y_val): This unpacking separates the loaded data into\n",
        "# four variables:\n",
        "# X_train: This variable stores the training data. It's a NumPy array where each\n",
        "# element represents a movie review. A review is essentially a sequence of integers,\n",
        "# where each integer represents a word based on a vocabulary.\n",
        "# y_train: This variable stores the sentiment labels for the training data. It's a\n",
        "# NumPy array where each element corresponds to a review in X_train and holds the\n",
        "# sentiment label (0 for negative, 1 for positive).\n",
        "# X_val: This variable stores the validation data, following the same format as\n",
        "# X_train. It's used to evaluate the model's performance during training.\n",
        "# y_val: This variable stores the sentiment labels for the validation data,\n",
        "# following the same format as y_train. It corresponds to the reviews in X_val.\n",
        "\n",
        "# Limiting Vocabulary Size (Optional Argument):\n",
        "\n",
        "# num_words=20000: This is an optional argument that specifies the maximum number\n",
        "# of words to consider in the vocabulary. By default, it considers the 20,000 most\n",
        "# frequent words in the dataset. This helps reduce the dimensionality of the data\n",
        "# and improve training efficiency.\n",
        "\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=20000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dQzOvvEFs5iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.- PADDING DATA"
      ],
      "metadata": {
        "id": "jQCP7Vk8s6ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DdUjLn_7Gs8E"
      },
      "outputs": [],
      "source": [
        "# This code snippet deals with \"padding sequences\"\n",
        "\n",
        "# 1. Setting Maximum Sequence Length:\n",
        "\n",
        "# max_length = 500: This line defines a variable named max_length and assigns it a\n",
        "# value of 500. This variable represents the maximum length (number of words) a\n",
        "# sequence (review) can have in the processed data.\n",
        "\n",
        "# 2. Padding Training Data:\n",
        "\n",
        "# X_train = pad_sequences(X_train, maxlen=max_length): This line applies the\n",
        "# pad_sequences function from TensorFlow's keras.preprocessing.sequence module to\n",
        "# the training data stored in X_train.\n",
        "# pad_sequences: This function takes two arguments:\n",
        "# The first argument (X_train) is the actual training data, which is likely a\n",
        "# NumPy array where each element represents a movie review. Each review itself is\n",
        "# a sequence of integers representing individual words based on a vocabulary.\n",
        "# The second argument (maxlen=max_length) specifies the maximum sequence length\n",
        "#  (set to 500 in this case).\n",
        "# The function's purpose is to ensure all sequences in the training data (X_train)\n",
        "# have the same length (max_length).\n",
        "\n",
        "# 3. Padding Validation Data:\n",
        "\n",
        "# X_val = pad_sequences(X_val, maxlen=max_length): This line follows the same logic\n",
        "# as the previous line, but it applies the pad_sequences function to the validation\n",
        "# data stored in X_val. This ensures all sequences in the validation set also have\n",
        "# the same length (max_length) for consistency with the training data.\n",
        "\n",
        "\n",
        "max_length = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_length)\n",
        "X_val = pad_sequences(X_val, maxlen=max_length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YOql5Ah5tDW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.- DATA SHAPE"
      ],
      "metadata": {
        "id": "aB2q0Jm3tEcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njM1qsDhHHax",
        "outputId": "e03a21fb-1e72-4c53-c0ac-64cfbd48ffd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 500) (25000, 500) (25000,) (25000,)\n"
          ]
        }
      ],
      "source": [
        "# Data shapes\n",
        "\n",
        "# Knowing the shapes of your data helps ensure everything is processed correctly.\n",
        "# It verifies that the dimensions of your training and validation data are\n",
        "# compatible for training your model.\n",
        "\n",
        "# There are 25,000 reviews in the training data (X_train).\n",
        "# There are 10,000 reviews in the validation data (X_val).\n",
        "# Each review (sequence) in both training and validation data has been padded to a\n",
        "# maximum length of 500 (specified earlier in the code).\n",
        "# y_train and y_val contain labels for each review, and they likely have a single\n",
        "# dimension representing the sentiment (0 or 1). The number of elements in these\n",
        "# arrays should match the number of reviews in the corresponding training and\n",
        "# validation sets (X_train and X_val).\n",
        "# By printing the shapes, you can confirm that the padding process worked as\n",
        "# expected and your data is ready for further processing in your sentiment analysis\n",
        "# model.\n",
        "\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3L3ri_5ZtN8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.- DEFINING VOCABULARY SIZE, EMBEDDING DIMENSION AND MAXIMUM LENGTH"
      ],
      "metadata": {
        "id": "io2-70W9tPJU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9x_9TKgsHRs5"
      },
      "outputs": [],
      "source": [
        "# This snippet defines three important variables used in building a sentiment\n",
        "# analysis model for the IMDB dataset:\n",
        "\n",
        "# vocab_size = 20000:\n",
        "\n",
        "# This variable defines the vocabulary size, which is the number of unique words\n",
        "# considered in the model. Here, it's set to 20,000.\n",
        "\n",
        "# embedding_dim = 100:\n",
        "\n",
        "# This variable defines the embedding dimension. In sentiment analysis, each word\n",
        "# in a review gets converted into a numerical vector representation. This embedding\n",
        "# dimension specifies the size (length) of these vectors. Here, each word is\n",
        "# represented by a 100-dimensional vector.\n",
        "\n",
        "# max_length = 500:\n",
        "\n",
        "# This variable defines the maximum length (number of words) for a review (sequence)\n",
        "# in the processed data. Here, it's set to 500.\n",
        "\n",
        "vocab_size = 20000\n",
        "embedding_dim = 100\n",
        "max_length = 500\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FDig9SPytpER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.- DEFINING THE ARCHITECTURE OF THE MODEL"
      ],
      "metadata": {
        "id": "xs4EW8KBtqXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LFP4cMphHdmU"
      },
      "outputs": [],
      "source": [
        "# This code defines the architecture of a neural network for sentiment analysis\n",
        "# on the IMDB dataset, using a combination of Convolutional Neural Networks (CNNs)\n",
        "# and Long Short-Term Memory (LSTM) networks. Here's a breakdown of each layer:\n",
        "\n",
        "# Input Layer (input_layer):\n",
        "\n",
        "# This layer defines the entry point for the network. It takes sequences of integers\n",
        "# representing words in a review. The .shape argument specifies the expected input\n",
        "# shape, which is a tuple (max_length,) in this case. This means the network expects\n",
        "# sequences with a maximum length of max_length (defined earlier).\n",
        "\n",
        "# Embedding Layer (embedding_layer):\n",
        "\n",
        "# This layer transforms integer-represented words into dense vectors. It takes\n",
        "# three arguments:\n",
        "# vocab_size: The number of unique words considered (set to vocab_size earlier).\n",
        "# embedding_dim: The dimensionality of the word vectors (set to embedding_dim earlier).\n",
        "# input_length (optional): Here, it's set to max_length to ensure the embedding\n",
        "# layer processes sequences of the expected length.\n",
        "# This layer essentially maps each word index to a corresponding vector representation,\n",
        "# capturing semantic relationships between words.\n",
        "\n",
        "# Convolutional Layer (conv_layer):\n",
        "\n",
        "# This layer is the first CNN layer. It applies one-dimensional convolutional\n",
        "# filters to the embedded sequences. The arguments are:\n",
        "# 128: The number of filters used in the convolution (extracts 128 features).\n",
        "# 5: The size of the filter window (considers 5 consecutive word vectors at a time).\n",
        "# activation='relu': The activation function applied to the convolution output\n",
        "#  (ReLU for non-linearity).\n",
        "# The goal of this layer is to capture local patterns in the sequence that might\n",
        "# be indicative of sentiment (e.g., presence of sentiment-laden phrases).\n",
        "\n",
        "# Pooling Layer (pooling_layer):\n",
        "\n",
        "# This layer performs downsampling on the output of the convolutional layer. Here,\n",
        "# MaxPooling1D takes the maximum value from a window of size 4 (keeps the most\n",
        "# significant feature from every 4 consecutive outputs of the convolutional layer).\n",
        "# This reduces the dimensionality of the data and helps control overfitting.\n",
        "\n",
        "# LSTM Layer (lstm_layer):\n",
        "\n",
        "# This layer introduces an LSTM network. LSTMs are powerful for handling sequential\n",
        "# data like reviews. It takes the output of the pooling layer and processes the\n",
        "# sequence to capture long-range dependencies in the word order. The argument is:\n",
        "# 128: The number of units in the LSTM layer (defines the internal memory of the LSTM).\n",
        "# LSTMs can learn how the sentiment of a review might be influenced by words further\n",
        "# apart in the sequence (e.g., \"not bad\" vs. \"bad not\").\n",
        "\n",
        "# Dense Layer (dense_layer):\n",
        "\n",
        "# This layer is a fully-connected layer that transforms the LSTM output into a\n",
        "# lower-dimensional space. It has:\n",
        "# 64: The number of neurons in the dense layer.\n",
        "# activation='relu': The activation function applied (ReLU for non-linearity).\n",
        "# This layer helps extract higher-level features from the sequence data.\n",
        "\n",
        "# Dropout Layer (dropout_layer):\n",
        "\n",
        "# This layer introduces dropout regularization. It randomly drops a certain\n",
        "# percentage of neurons (here, 50%) during training to prevent overfitting.\n",
        "# The argument is:\n",
        "# 0.5: The dropout rate (50% of neurons are dropped).\n",
        "\n",
        "# Output Layer (output_layer):\n",
        "\n",
        "# This layer is the final layer of the network. It has:\n",
        "# 1: The number of neurons (as the task is binary sentiment classification: positive\n",
        "# or negative).\n",
        "# activation='sigmoid': The activation function applied (sigmoid for binary\n",
        "# classification, outputting a value between 0 and 1 representing the probability\n",
        "# of positive sentiment).\n",
        "# This layer outputs the final prediction, a probability score indicating the\n",
        "# sentiment of the review (closer to 1 for positive, closer to 0 for negative).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_layer = Input(shape=(max_length,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_layer)\n",
        "conv_layer = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
        "pooling_layer = MaxPooling1D(pool_size=4)(conv_layer)\n",
        "lstm_layer = LSTM(128)(pooling_layer)\n",
        "dense_layer = Dense(64, activation='relu')(lstm_layer)\n",
        "dropout_layer = Dropout(0.5)(dense_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sRPz3Sx0t-DQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.- MODEL COMPILATION"
      ],
      "metadata": {
        "id": "9kWRbewxt_UA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ6M6W_iHhOA",
        "outputId": "d4573006-6913-4c68-85db-0a5908da349a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 500)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 500, 100)          2000000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 496, 128)          64128     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 124, 128)          0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2204033 (8.41 MB)\n",
            "Trainable params: 2204033 (8.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# This snippet defines the final steps for building and compiling the sentiment\n",
        "# analysis model using the architecture defined earlier:\n",
        "\n",
        "# 1. Model Creation (model = Model(inputs=input_layer, outputs=output_layer)):\n",
        "\n",
        "# This line creates a Model object using the tensorflow.keras.models.Model class.\n",
        "# It takes two arguments:\n",
        "# inputs: This specifies the input layer of the model, which is the input_layer\n",
        "# defined earlier.\n",
        "# outputs: This specifies the output layer of the model, which is the output_layer\n",
        "# defined earlier.\n",
        "# Essentially, this line constructs the overall neural network architecture based\n",
        "# on the sequence of layers you defined previously.\n",
        "\n",
        "# 2. Model Compilation (model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "# metrics=['accuracy'])):\n",
        "\n",
        "# This line compiles the model, which configures it for training. It takes three\n",
        "# arguments:\n",
        "# loss: This specifies the loss function used to measure the difference between\n",
        "# the model's predictions and the true labels. Here, 'binary_crossentropy' is used\n",
        "# because it's a binary classification task (positive or negative sentiment).\n",
        "# optimizer: This specifies the optimization algorithm used to train the model.\n",
        "# Here, 'adam' is a popular optimizer choice for its efficiency.\n",
        "# metrics: This is a list of metrics used to evaluate the model's performance\n",
        "# during training and validation. Here, 'accuracy' is used to track the percentage\n",
        "# of correct predictions.\n",
        "\n",
        "# 3. Model Summary (model.summary()):\n",
        "\n",
        "# This line calls the summary method on the model object. This method prints a\n",
        "# summary of the model's architecture, including:\n",
        "# Layer names and types\n",
        "# Number of parameters in each layer\n",
        "# Total number of trainable parameters\n",
        "# Output shape\n",
        "# Overall, this code snippet completes the model definition by creating the model\n",
        "# object, configuring its training process, and providing insights into its\n",
        "# architecture through the summary.\n",
        "\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r7McloskuFdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.-TRAINING DATA"
      ],
      "metadata": {
        "id": "kcxXfoyFuGFU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJaFhBQIHk_m",
        "outputId": "0dc5d1dc-4ace-437c-8192-fad1d662db6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 316s 799ms/step - loss: 0.3894 - accuracy: 0.8092 - val_loss: 0.2646 - val_accuracy: 0.8923\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 328s 840ms/step - loss: 0.1657 - accuracy: 0.9430 - val_loss: 0.3012 - val_accuracy: 0.8752\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 327s 835ms/step - loss: 0.0854 - accuracy: 0.9725 - val_loss: 0.3618 - val_accuracy: 0.8732\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 329s 842ms/step - loss: 0.0360 - accuracy: 0.9894 - val_loss: 0.5835 - val_accuracy: 0.8707\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 362s 926ms/step - loss: 0.0271 - accuracy: 0.9914 - val_loss: 0.5560 - val_accuracy: 0.8700\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 317s 810ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.7136 - val_accuracy: 0.8684\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 327s 837ms/step - loss: 0.0151 - accuracy: 0.9953 - val_loss: 0.7003 - val_accuracy: 0.8627\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 324s 830ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.6549 - val_accuracy: 0.8672\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 324s 830ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.6381 - val_accuracy: 0.8606\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 322s 825ms/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.8321 - val_accuracy: 0.8679\n"
          ]
        }
      ],
      "source": [
        "# Training Data:\n",
        "\n",
        "# The first two arguments specify the training data:\n",
        "# X_train: This is the NumPy array containing the padded sequences (reviews)\n",
        "# representing the training data.\n",
        "# y_train: This is the NumPy array containing the sentiment labels (0 for negative,\n",
        "# 1 for positive) for each review in X_train.\n",
        "\n",
        "# Training Parameters:\n",
        "\n",
        "# The next two arguments define training parameters:\n",
        "# epochs=10: This specifies the number of times the entire training dataset will\n",
        "# be passed through the network for training. Here, the model will be trained for\n",
        "# 10 epochs.\n",
        "# batch_size=64: This specifies the number of samples used to update the model's\n",
        "# weights in one iteration. Here, the model will update its weights after processing\n",
        "# batches of 64 reviews.\n",
        "\n",
        "# Validation Data (Optional):\n",
        "\n",
        "# The final argument, validation_data=(X_val, y_val)), is optional but highly\n",
        "# recommended. It specifies the validation data:\n",
        "# X_val: This is the NumPy array containing the padded sequences (reviews)\n",
        "# representing the validation data.\n",
        "# y_val: This is the NumPy array containing the sentiment labels (0 for negative,\n",
        "# 1 for positive) for each review in X_val.\n",
        "\n",
        "trained_model = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UQWhYWdJuQ6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.-MODEL EVALUATION"
      ],
      "metadata": {
        "id": "LO5WrS7muSZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "al5B4IKqHoHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0e36af-ee64-4e29-ce21-f5cc9b1074f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 88s 113ms/step - loss: 0.8321 - accuracy: 0.8679\n",
            "Loss: 0.8321132063865662\n",
            "Accuracy: 0.8678799867630005\n"
          ]
        }
      ],
      "source": [
        "# This snippet evaluates the performance of the trained sentiment analysis model\n",
        "# on the validation data and then prints the results. Here's a breakdown:\n",
        "\n",
        "# Evaluation:\n",
        "\n",
        "# loss, accuracy = model.evaluate(X_val, y_val): This line calls the evaluate\n",
        "\n",
        "# method on the trained model (model). It takes two arguments:\n",
        "\n",
        "# X_val: The NumPy array containing the padded sequences (reviews) representing\n",
        "# the validation data.\n",
        "# y_val: The NumPy array containing the sentiment labels (0 for negative, 1 for\n",
        "# positive) for each review in X_val.\n",
        "\n",
        "# The evaluate method performs a forward pass through the network using the\n",
        "# validation data and calculates two key metrics:\n",
        "\n",
        "# Loss: This quantifies the difference between the model's predictions and the true\n",
        "# labels (usually lower is better).\n",
        "\n",
        "# Accuracy: This represents the percentage of correct predictions made by the model\n",
        "# on the validation data (usually higher is better).\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretation\n",
        "# Loss on the validation set (0.8321): The loss is relatively high, suggesting\n",
        "# there is room for improvement in the model. A high loss on the validation set\n",
        "# compared to the training set indicates that the model may be overfitting.\n",
        "\n",
        "# Accuracy on the validation set (86.79%): An accuracy of 86.79% is quite good,\n",
        "# but given that the accuracy on the training set was extremely high (99.61%),\n",
        "# this difference also suggests overfitting.\n",
        "\n",
        "# The evaluation confirms that the model performs well in terms of accuracy on\n",
        "# the validation set, but the high loss compared to the accuracy on the training\n",
        "# set suggests overfitting. Here are some additional suggestions to address\n",
        "# overfitting:\n",
        "\n",
        "# 1.- Regularization:\n",
        "# Dropout: Increase the Dropout rate.\n",
        "# L2 Regularization: Add L2 regularization in the dense layers.\n",
        "\n",
        "# 2.- Early Stopping: Implement EarlyStopping to stop training when the validation\n",
        "# loss stops improving.\n",
        "\n",
        "# 3.- Reduce the complexity of the model\n",
        "\n",
        "\n",
        "#It would take a very long time to train this model again after the changes\n",
        "#made\n"
      ],
      "metadata": {
        "id": "YnG1WBPq_7oo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}